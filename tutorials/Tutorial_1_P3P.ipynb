{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../\n",
    "%pip install -e .\n",
    "%cd tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5095824",
   "metadata": {},
   "source": [
    "# P3P (Perspective-3-Point) in torch : A Step-by-step tutorial with POSEIDON "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f4736",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to this interactive Jupyter Notebook!\n",
    "\n",
    "Pose estimation from minimal geometric constraints plays a central role in 3D computer vision. Among these, the **Perspective-Three-Point** (P3P) problem is a classic: given three 2D-3D point correspondences, estimate the camera's pose. Kneip's method provides a practical and reliable closed-form solution, making it a common choice in pose estimation tasks.\n",
    "\n",
    "- **Meet `POSEIDON`**, a fast, differentiable PyTorch library implementing Kneip‚Äôs P3P algorithm ‚Äî ideal for real-time, gradient-based learning.\n",
    "\n",
    "- Built on top of `autoroot`, it leverages differentiable polynomial solvers to bring exact geometry into your training loop.\n",
    "\n",
    "- Seamless autograd integration means you can embed POSEIDON directly into your loss function ‚Äî perfect for tasks like training a YOLO model that learns to predict 3D positions, minimizing reprojection or track alignment errors.\n",
    "\n",
    "Let‚Äôs take a look at how POSEIDON combines geometric precision with practical integration into modern pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/Pruneeuh/POSEIDON@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    !{sys.executable} -m pip install \"cmath\"\n",
    "    !{sys.executable} -m pip install \"matplotlib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f645c",
   "metadata": {},
   "source": [
    "### **Summarizing Kneip PEP Method for a Direct Computation of Absolute Camera Position and Orientation :** \n",
    "\n",
    "#### 1. Compute a transformation matrix (T) and the feature vector f3_T in features vectors frame  \n",
    "$ \\space\\space\\space\\space \\vec{f_i\\_T} = T. \\vec{fi} $\n",
    "#### 2. Compute a transformation matrix (N) and the world point P3_N in the world-point frame  \n",
    "$ \\space\\space\\space\\space P_i\\_N = N . (P_i - P_1) $\n",
    "#### 3. Extract p1 and p2 from P3_N  \n",
    "$ \\space\\space\\space\\space P_3\\_N = \\begin{pmatrix} p_1 \\\\ p_2 \\\\ 0 \\end{pmatrix} $\n",
    "#### 4. Compute d12 and b  \n",
    "$ \\space\\space\\space\\space b = cot \\beta = \\pm \\sqrt{ \\frac{1}{1 - cos(\\beta)^2} -1 } = \\pm \\sqrt{ \\frac{1}{1 - (\\vec{f1}.\\vec{f2})^2} -1 }$\n",
    "#### 5. Compute phi1 and phi2  \n",
    "$ \\space\\space\\space\\space \\phi_1 = \\frac{(f_3\\_T)_x}{(f_3\\_T)_z} \\space  and \\space \\phi_2 = \\frac{(f_3\\_T)_y}{(f_3\\_T)_z} $\n",
    "#### 6. Compute the factors a4,a3,a2,a1 and a0 of polynomial  \n",
    "$ \\space\\space\\space\\space a4 cos^4\\theta+a3 cos^3\\theta+a2 cos^2\\theta + a1 cos\\theta +a0 = 0 $\n",
    "#### 7. Find the real roots of the polynomial (values for cos_teta)  \n",
    "Using the library [autoroot](https://github.com/Pruneeuh/autoroot)\n",
    "#### 8. For each solutions find the values for cot_alpha  \n",
    "$ \\space\\space\\space\\space cot\\alpha = \\frac{\\frac{\\phi_1}{\\phi_2}.p1 + cos\\theta.p2 - d_{12}.b}{\\frac{\\phi_1}{\\phi_2}.p2 - p1 +d_{12}} $\n",
    "#### 9. Compute all necessary trigonometric forms of alpha and teta using trigonometric relationships and the restricted parameter domains  \n",
    "$ \\space\\space\\space\\space cos\\theta = \\real{(root)} \\\\ \n",
    " \\space\\space\\space\\space sin\\alpha = \\sqrt{ \\frac{1}{cot^2\\alpha +1}} \\\\\n",
    " \\space\\space\\space\\space sin\\theta = \\pm \\sqrt{1 - cos^2\\theta} \\\\ \n",
    " \\space\\space\\space\\space cos\\alpha = \\pm \\sqrt{1-sin^2\\alpha}  \\\\ $\n",
    "#### 10. for each solution, compute C_ and Q  \n",
    "$ \\space\\space\\space\\space C\\_N = \\begin{pmatrix} \n",
    "d_{12}  \\space  cos\\alpha \\space (sin\\alpha .b + cos\\alpha ) \\\\\n",
    "d_{12} \\space sin\\alpha \\space cos\\theta \\space(sin\\alpha .b + cos\\alpha ) \\\\\n",
    "d_{12} \\space sin\\alpha \\space sin\\theta \\space (sin\\alpha .b + cos\\alpha )\n",
    "\\end{pmatrix}\n",
    "\\space\\space\\space\\space\n",
    "Q = \\begin{pmatrix}\n",
    "-cos\\alpha & -sin\\alpha \\space cos\\theta & - sin\\alpha \\space sin\\theta \\\\\n",
    "sin\\alpha & -cos\\alpha \\space cos\\theta & -cos\\alpha \\space sin\\theta \\\\\n",
    "0 & -sin \\theta & cos\\theta\n",
    "\\end{pmatrix}\n",
    "$\n",
    "#### 11. for each solution, compute the absolute camera center C and orientation R  \n",
    "$ \\space\\space\\space\\space  C = P_1 + N^T . C\\_N \\\\\n",
    "\\space\\space\\space\\space R = N^T . Q^T . T $\n",
    "#### 12. Backproject a fourth point for disambiguation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691258f4",
   "metadata": {},
   "source": [
    "## Using P3P with POSEIDON\n",
    "\n",
    "Now, let's use your `poseidon` library to compute the positon and rotation matrix of the camera \n",
    "\n",
    "First, ensure you have `poseidon` installed : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, uncomment\n",
    "# !pip install poseidon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbd4cd",
   "metadata": {},
   "source": [
    "### To use the **P3P (Perspective-Three-Point)** method, the following components are required :\n",
    "#### 1. Camera pose : rotation and position \n",
    "- The ground truht camera is composed of \n",
    "    * **Rotation matrix (R)**, which defines the orientation of the camera in world coordinates.\n",
    "    * **A camera position (C)**,  which defines the camera's location in the world.\n",
    "- To test the P3P method under various conditions, you can generate random camera poses, including different positions, orientations, and intrinsic parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96680621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.numpy import (\n",
    "    generate_position_matrix,\n",
    "    generate_rotation_matrix,\n",
    ")\n",
    "from poseidon.torch import convert_matrix_numpy_to_batch\n",
    "\n",
    "R_np = generate_rotation_matrix()\n",
    "C_np = generate_position_matrix()\n",
    "\n",
    "# convert the numpy matrices to batch format\n",
    "R = convert_matrix_numpy_to_batch(R_np)\n",
    "C = convert_matrix_numpy_to_batch(C_np)\n",
    "\n",
    "print(\"Rotation matrix (R):\")\n",
    "print(R, R.shape)\n",
    "print(\"Camera position (C):\")\n",
    "print(C, C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd5a36",
   "metadata": {},
   "source": [
    "#### 2. Camera intrinsics \n",
    "- The **intrinsic matrix (A)** models the internal parameters of the camera : \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix} $\n",
    "    * $f_x, f_y$ are the focal lengths (in pixel)\n",
    "    *  $c_x, c_y$ is the principal points  \n",
    "- This matrix is used to project 3D points into 2D image coordinates, and is important if you want to simulate or reverse perspective projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9da690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.numpy import generate_camera_parameters\n",
    "\n",
    "A_np = generate_camera_parameters()\n",
    "A = convert_matrix_numpy_to_batch(A_np)\n",
    "\n",
    "print(\"Camera intrinsic parameters (A):\")\n",
    "print(A, A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86cc5b",
   "metadata": {},
   "source": [
    "#### 3. Four 3D points \n",
    "- These are real-world coordinates of three points visible from the camera. \n",
    "- Here we are only generating one serie of 4 points for the example (but it is possible to generate tensors the size of a defined batch, see tutorial 3).\n",
    "- The P3P algorithm itself only requires 3 points, but we use a 4th point to disambiguate and retain only one solution among the candidates that P3P may return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a04b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from poseidon.numpy import generate_points_3D\n",
    "\n",
    "points_3D_np = generate_points_3D()\n",
    "\n",
    "points_3D = convert_matrix_numpy_to_batch(points_3D_np)  # convert numpy array to batch format\n",
    "\n",
    "print(\"3D points : \\n\", points_3D, points_3D.shape)\n",
    "\n",
    "\n",
    "# Create of the 3D figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the camera center\n",
    "ax.scatter(C[:, 0].squeeze(), C[:, 1].squeeze(), C[:, 2].squeeze(), color=\"orange\")\n",
    "\n",
    "# Plot the features points\n",
    "for i in range(4):\n",
    "    Pi = points_3D[:, i].squeeze()\n",
    "    ax.scatter(*Pi, color=\"black\")\n",
    "    ax.text(*Pi, f\"$P_{i+1}$\")\n",
    "    ax.plot(\n",
    "        [C[:, 0].squeeze(), Pi[0]], [C[:, 1].squeeze(), Pi[1]], [C[:, 2].squeeze(), Pi[2]], \"k--\"\n",
    "    )\n",
    "\n",
    "# Set axis labels and limits\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "\n",
    "# Plot the figure\n",
    "ax.set_title(\"The four 3D Points use for P3P\")\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5bf88",
   "metadata": {},
   "source": [
    "#### 4. The projection of the 2D points \n",
    "- The 2D points correspond to the projection of the 3D points onto the image plane, using the full camera model.\n",
    "- This involves both the extrinsic parameters (rotation matrix (R), camera position (C) and the intrinsic matrix (ùê¥)).\n",
    "- They are going to be used to find the best solution after the apply of the P3P algorithm. \n",
    "- The projection follows this process:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5814f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import generate_synthetic_2D3Dpoints\n",
    "\n",
    "points_2D = generate_synthetic_2D3Dpoints(R, C, A, points_3D)\n",
    "print(\"2D points:\\n\", points_2D, points_2D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1e25c",
   "metadata": {},
   "source": [
    "#### 5. Features vectors\n",
    "- These are unit vectors pointing from the camera center toward the observed 3 feature points (2D projections).\n",
    "- They represent the directions in the camera frame corresponding to the image observations of the 3D world points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import get_feature_vectors\n",
    "\n",
    "features_vectors = get_feature_vectors(points_2D[:, :3], A)\n",
    "print(\"Features vectors:\\n\", features_vectors, features_vectors.shape)\n",
    "# Warning: Only the first 3 points are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Create of the 3D figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the camera center\n",
    "ax.scatter(C[:, 0].squeeze(), C[:, 1].squeeze(), C[:, 2].squeeze(), color=\"orange\")\n",
    "\n",
    "ft_sq = features_vectors.squeeze(0).numpy()\n",
    "\n",
    "# Plot the features points\n",
    "for i in range(3):\n",
    "    Pi = points_3D[:, i].squeeze()\n",
    "    print(f\"P{i+1} position:\", Pi)\n",
    "    ax.scatter(*Pi, color=\"black\")\n",
    "\n",
    "    textpoint = C.squeeze() - (C.squeeze() - Pi) / 7\n",
    "    ax.text(*textpoint, f\"$P_{i+1}$\")\n",
    "    ax.plot(\n",
    "        [C[:, 0].squeeze(), Pi[0]], [C[:, 1].squeeze(), Pi[1]], [C[:, 2].squeeze(), Pi[2]], \"k--\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot the feature vectors\n",
    "scale = 1\n",
    "ax.quiver(\n",
    "    C[:, 0].item(),\n",
    "    C[:, 1].item(),\n",
    "    C[:, 2].item(),\n",
    "    ft_sq[:, 0] * scale,\n",
    "    ft_sq[:, 1] * scale,\n",
    "    ft_sq[:, 2] * scale,\n",
    "    color=[\"r\", \"g\", \"b\"],\n",
    ")\n",
    "\n",
    "# Add legend for the feature vectors\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"r\", lw=2, label=\"f1\"),\n",
    "    Line2D([0], [0], color=\"g\", lw=2, label=\"f2\"),\n",
    "    Line2D([0], [0], color=\"b\", lw=2, label=\"f3\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "\n",
    "# Set axis labels and limits\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_xlim([C[:, 0].item() - 5, C[:, 0].item() + 5])\n",
    "ax.set_ylim([C[:, 1].item() - 5, C[:, 1].item() + 5])\n",
    "ax.set_zlim([C[:, 2].item() - 5, C[:, 2].item() + 5])\n",
    "\n",
    "# Plot the figure\n",
    "ax.set_title(\"The three 3D features vectors\")\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef1884",
   "metadata": {},
   "source": [
    "### Now we can apply the P3P algorithm \n",
    "- This algorithm returns a solution matrix where, for each batch, it provides four possible solutions.\n",
    "- This algorithm takes as input the previously determined feature vectors and the 3D points.\n",
    "- The solution is stored as a 4-layer matrix : for each layer the first column represents the translation vector, and the remaining three columns correspond to the rotation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41680e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import P3P\n",
    "\n",
    "solutions = P3P(points_3D, features_vectors)\n",
    "\n",
    "print(\"P3P solutions:\\n\", solutions)\n",
    "print(\"Camera position (C):\\n\", C)\n",
    "print(\"Camera rotation (R):\\n\", R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36564811",
   "metadata": {},
   "source": [
    "### Find the best solution \n",
    "- The P3P algorithm yields multiple possible camera poses.  \n",
    "- To resolve this ambiguity, a fourth 2D-3D point correspondence is used to select the most accurate solution‚Äîspecifically, the one with the lowest reprojection error.\n",
    "- This algorithm returns the rotation and translation matrices that yield the smallest reprojection error, along with the reprojection error values for the four points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import find_best_solution_P3P_batch\n",
    "\n",
    "R_solution, C_solution, estimation_error = find_best_solution_P3P_batch(\n",
    "    solutions, points_2D, points_3D, A\n",
    ")\n",
    "\n",
    "print(\"\\033[1m Estimated Rotation Matrix \\033[0m (R_solution):\\n \", R_solution)\n",
    "print(\"\\033[1m Original Rotation Matrix \\033[0m  (R):\\n\", R, \"\\n\")\n",
    "print(\"\\033[1m Estimated Camera Position \\033[0m  (C_solution):\\n\", C_solution)\n",
    "print(\"\\033[1m Original Camera Position \\033[0m (C):\\n\", C, \"\\n\")\n",
    "print(\"Estimation Error:\\n\", estimation_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
