{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../\n",
    "%pip install -e .\n",
    "%cd tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5095824",
   "metadata": {},
   "source": [
    "# P3P (Perspective-3-Point) in torch : A Step-by-step tutorial with POSEIDON "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f4736",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to this interactive Jupyter Notebook!\n",
    "\n",
    "Pose estimation from minimal geometric constraints plays a central role in 3D computer vision. Among these, the **Perspective-Three-Point** (P3P) problem is a classic: given three 2D-3D point correspondences, estimate the camera's pose. Kneip's method provides a practical and reliable closed-form solution, making it a common choice in pose estimation tasks.\n",
    "\n",
    "- **Meet `POSEIDON`**, a fast, differentiable PyTorch library implementing Kneip’s P3P algorithm — ideal for real-time, gradient-based learning.\n",
    "\n",
    "- Built on top of `autoroot`, it leverages differentiable polynomial solvers to bring exact geometry into your training loop.\n",
    "\n",
    "- Seamless autograd integration means you can embed POSEIDON directly into your loss function — perfect for tasks like training a YOLO model that learns to predict 3D positions, minimizing reprojection or track alignment errors.\n",
    "\n",
    "Let’s take a look at how POSEIDON combines geometric precision with practical integration into modern pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/Pruneeuh/POSEIDON@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    !{sys.executable} -m pip install \"cmath\"\n",
    "    !{sys.executable} -m pip install \"matplotlib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f645c",
   "metadata": {},
   "source": [
    "Summarizing Kneip PEP Method for a Direct Computation of Absolute Camera Position and Orientation\n",
    "\n",
    "#### 1. Compute a transformation matrix (T) and the feature vector f3_T in features vectors frame  \n",
    "$ \\space\\space\\space\\space \\vec{f_i\\_T} = T. \\vec{fi} $\n",
    "#### 2. Compute a transformation matrix (N) and the world point P3_N in the world-point frame  \n",
    "$ \\space\\space\\space\\space P_i\\_N = N . (P_i - P_1) $\n",
    "#### 3. Extract p1 and p2 from P3_N  \n",
    "$ \\space\\space\\space\\space P_3\\_N = \\begin{pmatrix} p_1 \\\\ p_2 \\\\ 0 \\end{pmatrix} $\n",
    "#### 4. Compute d12 and b  \n",
    "$ \\space\\space\\space\\space b = cot \\beta = \\pm \\sqrt{ \\frac{1}{1 - cos(\\beta)^2} -1 } = \\pm \\sqrt{ \\frac{1}{1 - (\\vec{f1}.\\vec{f2})^2} -1 }$\n",
    "#### 5. Compute phi1 and phi2  \n",
    "$ \\space\\space\\space\\space \\phi_1 = \\frac{(f_3\\_T)_x}{(f_3\\_T)_z} \\space  and \\space \\phi_2 = \\frac{(f_3\\_T)_y}{(f_3\\_T)_z} $\n",
    "#### 6. Compute the factors a4,a3,a2,a1 and a0 of polynomial  \n",
    "$ \\space\\space\\space\\space a4 cos^4\\theta+a3 cos^3\\theta+a2 cos^2\\theta + a1 cos\\theta +a0 = 0 $\n",
    "#### 7. Find the real roots of the polynomial (values for cos_teta)  \n",
    "Using the library [autoroot](https://github.com/Pruneeuh/autoroot) : \n",
    "#### 8. For each solutions find the values for cot_alpha  \n",
    "$ \\space\\space\\space\\space cot\\alpha = \\frac{\\frac{\\phi_1}{\\phi_2}.p1 + cos\\theta.p2 - d_{12}.b}{\\frac{\\phi_1}{\\phi_2}.p2 - p1 +d_{12}} $\n",
    "#### 9. Compute all necessary trigonometric forms of alpha and teta using trigonometric relationships and the restricted parameter domains  \n",
    "$ \\space\\space\\space\\space cos\\theta = \\real{(root)} \\\\ \n",
    " \\space\\space\\space\\space sin\\alpha = \\sqrt{ \\frac{1}{cot^2\\alpha +1}} \\\\\n",
    " \\space\\space\\space\\space sin\\theta = \\pm \\sqrt{1 - cos^2\\theta} \\\\ \n",
    " \\space\\space\\space\\space cos\\alpha = \\pm \\sqrt{1-sin^2\\alpha}  \\\\ $\n",
    "#### 10. for each solution, compute C_ and Q  \n",
    "$ \\space\\space\\space\\space C\\_N = \\begin{pmatrix} \n",
    "d_{12}  \\space  cos\\alpha \\space (sin\\alpha .b + cos\\alpha ) \\\\\n",
    "d_{12} \\space sin\\alpha \\space cos\\theta \\space(sin\\alpha .b + cos\\alpha ) \\\\\n",
    "d_{12} \\space sin\\alpha \\space sin\\theta \\space (sin\\alpha .b + cos\\alpha )\n",
    "\\end{pmatrix}\n",
    "\\space\\space\\space\\space\n",
    "Q = \\begin{pmatrix}\n",
    "-cos\\alpha & -sin\\alpha \\space cos\\theta & - sin\\alpha \\space sin\\theta \\\\\n",
    "sin\\alpha & -cos\\alpha \\space cos\\theta & -cos\\alpha \\space sin\\theta \\\\\n",
    "0 & -sin \\theta & cos\\theta\n",
    "\\end{pmatrix}\n",
    "$\n",
    "#### 11. for each solution, compute the absolute camera center C and orientation R  \n",
    "$ \\space\\space\\space\\space  C = P_1 + N^T . C\\_N \\\\\n",
    "\\space\\space\\space\\space R = N^T . Q^T . T $\n",
    "#### 12. Backproject a fourth point for disambiguation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ccf8e",
   "metadata": {},
   "source": [
    "Illustration : a faire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691258f4",
   "metadata": {},
   "source": [
    "## Using P3P with POSEIDON\n",
    "\n",
    "Now, let's use your `poseidon` library to compute the positon and rotation matrix of the camera \n",
    "\n",
    "First, ensure you have `poseidon` installed : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, uncomment\n",
    "# !pip install poseidon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbd4cd",
   "metadata": {},
   "source": [
    "### To use the **P3P (Perspective-Three-Point)** method, the following components are required :\n",
    "#### 1. Camera pose : rotation and position \n",
    "- The ground truht camera is composed of \n",
    "    * **Rotation matrix (R)**, which defines the orientation of the camera in world coordinates.\n",
    "    * **A camera position (C)**,  which defines the camera's location in the world.\n",
    "- To test the P3P method under various conditions, you can generate random camera poses, including different positions, orientations, and intrinsic parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96680621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.numpy import (\n",
    "    generate_position_matrix,\n",
    "    generate_rotation_matrix,\n",
    ")\n",
    "from poseidon.torch import convert_matrix_numpy_to_batch\n",
    "\n",
    "R_np = generate_rotation_matrix()\n",
    "C_np = generate_position_matrix()\n",
    "\n",
    "# convert the numpy matrices to batch format\n",
    "R = convert_matrix_numpy_to_batch(R_np)\n",
    "C = convert_matrix_numpy_to_batch(C_np)\n",
    "\n",
    "print(\"Rotation matrix (R):\")\n",
    "print(R, R.shape)\n",
    "print(\"Camera position (C):\")\n",
    "print(C, C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd5a36",
   "metadata": {},
   "source": [
    "#### 2. Camera intrinsics \n",
    "- The **intrinsic matrix (A)** models the internal parameters of the camera : \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix} $\n",
    "    * $f_x, f_y$ are the focal lengths (in pixel)\n",
    "    *  $c_x, c_y$ is the principal points  \n",
    "- This matrix is used to project 3D points into 2D image coordinates, and is important if you want to simulate or reverse perspective projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9da690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.numpy import generate_camera_parameters\n",
    "\n",
    "A_np = generate_camera_parameters()\n",
    "A = convert_matrix_numpy_to_batch(A_np)\n",
    "\n",
    "print(\"Camera intrinsic parameters (A):\")\n",
    "print(A, A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86cc5b",
   "metadata": {},
   "source": [
    "#### 3. Three 3D points \n",
    "- These are real-world coordinates of three points visible from the camera. \n",
    "- Here we are only generating one serie of 3 points for the example (but it is possible to generate tensors the size of a defined batch, see tutorial 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a04b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from poseidon.numpy import generate_points_3D\n",
    "\n",
    "points_3D_np = generate_points_3D()\n",
    "\n",
    "points_3D = convert_matrix_numpy_to_batch(points_3D_np)  # convert numpy array to batch format\n",
    "\n",
    "print(\"3D points : \\n\", points_3D, points_3D.shape)\n",
    "\n",
    "\n",
    "# Create of the 3D figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "ax.scatter(C[:, 0].squeeze(), C[:, 1].squeeze(), C[:, 2].squeeze(), color=\"orange\")\n",
    "\n",
    "for i in range(3):\n",
    "    Pi = points_3D[:, i].squeeze()\n",
    "    ax.scatter(*Pi, color=\"black\")\n",
    "    ax.text(*Pi, f\"$P_{i+1}$\")\n",
    "    ax.plot(\n",
    "        [C[:, 0].squeeze(), Pi[0]], [C[:, 1].squeeze(), Pi[1]], [C[:, 2].squeeze(), Pi[2]], \"k--\"\n",
    "    )\n",
    "    f = Pi - C\n",
    "\n",
    "# Set axis limits to visualize a 4x4x4 space centered around zero\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(\"The three 3D Points use for P3P\")\n",
    "\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5bf88",
   "metadata": {},
   "source": [
    "#### 5. The projection of the 2D points \n",
    "- The 2D points correspond to the projection of the 3D points onto the image plane, using the full camera model.\n",
    "- This involves both the extrinsic parameters (rotation matrix (R), camera position (C) and the intrinsic matrix (𝐴)).\n",
    "- They are going to be used to find the best solution after the apply of the P3P algorithm. \n",
    "- The projection follows this process:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5814f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import projection_all_point3D_to2D\n",
    "\n",
    "points_2D = projection_all_point3D_to2D(points_3D, C, R, A)\n",
    "print(\"2D points:\\n\", points_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_synthetic_2D3Dpoints_alya(R, C, A, P1, P2, P3):\n",
    "    print(\"P1\", P1, \"\\nP2\", P2, \"\\nP3\", P3)\n",
    "    \"\"\"\n",
    "    Generate synthetic corresponding 2D and 3D points for P3P problem.\n",
    "    Args:\n",
    "        R (torch.Tensor): Rotation matrix (3x3).\n",
    "        C (torch.Tensor): Camera center (3,).\n",
    "        A (torch.Tensor): Camera intrinsic matrix (3x3).\n",
    "        P1, P2, P3 (list): 3D points in world coordinates (3,).\n",
    "    Returns:\n",
    "        points2D (torch.Tensor): Projected 2D points in image coordinates (3x2).\n",
    "    \"\"\"\n",
    "    points3D = torch.tensor([P1, P2, P3], dtype=torch.float64).T\n",
    "    print(\"3D points (shape 3x3):\\n\", points3D)\n",
    "\n",
    "    # Compute camera translation vector from rotation R and position C\n",
    "    t = -R @ torch.reshape(C, (3, 1))  # (3, 1)\n",
    "    print(\"Camera translation vector (t):\\n\", t)\n",
    "\n",
    "    # Build projection matrix: P = K [R|t]\n",
    "    Rt = torch.cat([R, t], dim=1)\n",
    "    print(\" Rt (shape 3x4):\\n\", Rt)\n",
    "    P = A @ Rt\n",
    "    print(\" P (shape 3x4):\\n\", P)\n",
    "\n",
    "    # Convert 3D points to homogeneous coordinates (4x3)\n",
    "    points3D_h = torch.cat([points3D, torch.ones(1, 3, dtype=torch.float64)], dim=0)\n",
    "    print(\"points3D_h:\\n\", points3D_h)\n",
    "    # Project 3D points to 2D image plane using projection matrix\n",
    "    proj = P @ points3D_h\n",
    "    proj = proj / proj[2, :]  # normalize homogeneous coordinates\n",
    "    print(\"Projected points (shape 3x3):\\n\", proj, proj.shape)\n",
    "    # Extract 2D image coordinates (3 points, shape 3x2)\n",
    "    points2D = proj[:2, :].T\n",
    "    print(\"Projected 2D points (shape 3x2):\\n\", points2D)\n",
    "\n",
    "    return points2D\n",
    "\n",
    "\n",
    "print(\"3D\", points_3D_np)\n",
    "\n",
    "pt_2D_alya = generate_synthetic_2D3Dpoints_alya(\n",
    "    R.squeeze(0), C.squeeze(0), A.squeeze(0), points_3D_np[0], points_3D_np[1], points_3D_np[2]\n",
    ")\n",
    "print(\"2D points from Alya:\\n\", pt_2D_alya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_2D3Dpoints(R, C, A, P1, P2, P3):\n",
    "    \"\"\"\n",
    "    Generate synthetic corresponding 2D and 3D points for P3P problem.\n",
    "    Args:\n",
    "        R (torch.Tensor): Rotation matrix (batch_size,3,3).\n",
    "        C (torch.Tensor): Camera center (batch_size,3).\n",
    "        A (torch.Tensor): Camera intrinsic matrix (batch_size,3,3).\n",
    "        P1, P2, P3 (list): 3D points in world coordinates (batch_size,3).\n",
    "    Returns:\n",
    "        points2D (torch.Tensor): Projected 2D points in image coordinates (batch_size,3,2).\n",
    "    \"\"\"\n",
    "    batch_size = R.shape[0]  # Get the batch size from the first dimension of R\n",
    "\n",
    "    points3D = torch.stack([P1, P2, P3], dim=-1)  # Shape (batch_size,3, 3)\n",
    "\n",
    "    # Compute camera translation vector from rotation R and position C\n",
    "    t = torch.matmul(-R, torch.reshape(C, (batch_size, 3, 1)))  # ( batch_size, 3, 1)\n",
    "\n",
    "    # Build projection matrix: P = A [R|t]\n",
    "    Rt = torch.cat([R, t], dim=2)  # (batch_size, 3, 4)\n",
    "\n",
    "    P = torch.matmul(A, Rt)  # (batch_size, 3, 4)\n",
    "\n",
    "    # Convert 3D points to homogeneous coordinates (4x3)\n",
    "    points3D_h = torch.cat(\n",
    "        [points3D, torch.ones(batch_size, 1, 3, dtype=torch.float64)], dim=1\n",
    "    )  # (batch_size, 4, 3)\n",
    "\n",
    "    # Project 3D points to 2D image plane using projection matrix\n",
    "    proj = torch.matmul(P, points3D_h)  # (batch_size, 3, 3)\n",
    "\n",
    "    proj = proj / proj[:, 2, :]  # normalize homogeneous coordinates\n",
    "\n",
    "    # Extract 2D image coordinates (3 points, shape 3x2)\n",
    "    points2D = proj[:, :2, :].transpose(1, 2)  # (batch_size, 3, 2)\n",
    "\n",
    "    return points2D\n",
    "\n",
    "\n",
    "pt = generate_synthetic_2D3Dpoints(R, C, A, points_3D[:, 0], points_3D[:, 1], points_3D[:, 2])\n",
    "print(\"2D points:\\n\", pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1e25c",
   "metadata": {},
   "source": [
    "#### 4. Features vectors\n",
    "- These are unit vectors pointing from the camera center toward each of the 3D points.\n",
    "- They represent how the camera sees each point in its reference frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8306fe7",
   "metadata": {},
   "source": [
    "**Alya version**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# after generating 2D and 3D points using generate_synthetic_2D3Dpoints, we can compute the feature vectors\n",
    "\n",
    "\n",
    "def get_feature_vectors(points2D, A):\n",
    "    \"\"\"\n",
    "    Compute feature vectors from 2D points and intrinsic matrix.\n",
    "\n",
    "    Args:\n",
    "        points2D (torch.Tensor): 2D points in image coordinates (batch_size,3,2).\n",
    "        A (torch.Tensor): Camera intrinsic matrix (batch_size,3,3).\n",
    "\n",
    "    Returns:\n",
    "        featuresVect (torch.tensor):  feature vectors for each point (batch_size, 3, 3).)\n",
    "    \"\"\"\n",
    "    batch_size = points2D.shape[0]\n",
    "    ones = torch.ones((batch_size, 3, 1), dtype=torch.float64)  # (batch_size, 3, 1)\n",
    "\n",
    "    # Convert to homogeneous coordinates: (x, y) → (x, y, 1)\n",
    "    p_h = torch.cat([points2D, ones], dim=-1)  # (batch_size, 3, 3)\n",
    "\n",
    "    A_inv = torch.linalg.inv(A)  # Inverse of intrinsic matrix\n",
    "\n",
    "    featuresVect = []\n",
    "\n",
    "    for i in range(3):\n",
    "        p_h_i = p_h[:, i].unsqueeze(-1)  # (batch_size, 3,1)\n",
    "\n",
    "        # Apply inverse of intrinsic matrix to get direction vector in camera frame\n",
    "        fi = torch.matmul(A_inv, p_h_i).squeeze(-1)  # (batch_size, 3)\n",
    "\n",
    "        # Normalize to get a unit vector (bearing direction)\n",
    "        fi = fi / torch.norm(fi, dim=1, keepdim=True)  # Normalize along the first dimension\n",
    "\n",
    "        featuresVect.append(fi)  # Reshape to (batch_size, 3, 1)\n",
    "\n",
    "    # Stack into a matrix: shape (3, 3) where each column is f1, f2, f3\n",
    "    featuresVect = torch.stack(featuresVect, dim=1)  # (batch_size, 3, 3)\n",
    "    return featuresVect\n",
    "\n",
    "\n",
    "ft_alya_batch = get_feature_vectors(pt_2D_alya.unsqueeze(0), A)\n",
    "print(\"Feature vectors from 2D points:\\n\", ft_alya_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import compute_features_vectors\n",
    "\n",
    "features_vectors = compute_features_vectors(points_3D, C, R)\n",
    "print(\"Features vectors:\\n\", features_vectors, features_vectors.shape)\n",
    "\n",
    "\n",
    "# Create of the 3D figure\n",
    "print(\"3D points (torch):\\n\", points_3D, points_3D.shape)\n",
    "print(\"Camera position (C):\", C, C.shape)\n",
    "print(\"Feature vectors (ft):\", features_vectors, features_vectors.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "ax.scatter(C[:, 0].squeeze(), C[:, 1].squeeze(), C[:, 2].squeeze(), color=\"orange\")\n",
    "origin = np.array([C[:, 0].item(), C[:, 1].item(), C[:, 2].item()])\n",
    "\n",
    "\n",
    "ft_sq = features_vectors.squeeze(0).numpy()\n",
    "\n",
    "for i in range(3):\n",
    "    Pi = points_3D[:, i].squeeze()\n",
    "    print(f\"P{i+1} position:\", Pi)\n",
    "    ax.scatter(*Pi, color=\"black\")\n",
    "    ax.text(*Pi, f\"$P_{i+1}$\")\n",
    "    ax.plot(\n",
    "        [C[:, 0].squeeze(), Pi[0]], [C[:, 1].squeeze(), Pi[1]], [C[:, 2].squeeze(), Pi[2]], \"k--\"\n",
    "    )\n",
    "print(\"ft_sq[:,0]:\", ft_sq[:, 0])  # x composantes des vecteurs\n",
    "print(\"ft_sq[:,1]:\", ft_sq[:, 1])  # y composantes\n",
    "print(\"ft_sq[:,2]:\", ft_sq[:, 2])  # z composantes\n",
    "print(\"origin:\", origin)\n",
    "\n",
    "scale = 1\n",
    "\n",
    "ax.quiver(\n",
    "    C[:, 0].item(),\n",
    "    C[:, 1].item(),\n",
    "    C[:, 2].item(),\n",
    "    ft_sq[:, 0] * scale,\n",
    "    ft_sq[:, 1] * scale,\n",
    "    ft_sq[:, 2] * scale,\n",
    "    color=[\"r\", \"g\", \"b\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Set axis limits to visualize a 4x4x4 space centered around zero\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(\"The three 3D Points use for P3P\")\n",
    "\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import compute_features_vectors\n",
    "\n",
    "features_vectors = compute_features_vectors(points_3D, C, R)\n",
    "print(\"Features vectors:\\n\", features_vectors, features_vectors.shape)\n",
    "print(\"Feature vectors from 2D points:\\n\", ft_alya)\n",
    "\n",
    "# Create of the 3D figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "ax.scatter(C[:, 0].squeeze(), C[:, 1].squeeze(), C[:, 2].squeeze(), color=\"orange\")\n",
    "\n",
    "for i in range(3):\n",
    "    Pi = points_3D[:, i].squeeze()\n",
    "    ax.scatter(*Pi, color=\"black\")\n",
    "    ax.text(*Pi, f\"$P_{i+1}$\")\n",
    "    ax.plot(\n",
    "        [C[:, 0].squeeze(), Pi[0]], [C[:, 1].squeeze(), Pi[1]], [C[:, 2].squeeze(), Pi[2]], \"k--\"\n",
    "    )\n",
    "    f = Pi - C.squeeze()\n",
    "    ax.quiver(\n",
    "        C[:, 0].squeeze(),\n",
    "        C[:, 1].squeeze(),\n",
    "        C[:, 2].squeeze(),\n",
    "        f[0],\n",
    "        f[1],\n",
    "        f[2],\n",
    "        color=\"red\",\n",
    "        normalize=True,\n",
    "    )\n",
    "    ax.text(\n",
    "        (C[:, 0].squeeze() + Pi[0]) / 2,\n",
    "        (C[:, 1].squeeze() + Pi[1]) / 2,\n",
    "        (C[:, 2].squeeze() + Pi[2]) / 2,\n",
    "        f\"$f_{i+1}$\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Set axis limits to visualize a 4x4x4 space centered around zero\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.set_zlim([-2, 2])\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(\"The three 3D Points and features vectors use for P3P\")\n",
    "\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef1884",
   "metadata": {},
   "source": [
    "### Now we can apply the P3P algorithm \n",
    "- This algorithm returns a solution matrix where, for each batch, it provides four possible solutions.\n",
    "- This algorithm takes as input the previously determined feature vectors and the 3D points.\n",
    "- The solution is stored as a 4-layer matrix : for each layer the first column represents the translation vector, and the remaining three columns correspond to the rotation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41680e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import P3P\n",
    "\n",
    "solutions = P3P(points_3D, features_vectors)\n",
    "\n",
    "print(\"P3P solutions:\\n\", solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36564811",
   "metadata": {},
   "source": [
    "### Find the best solution \n",
    "- The P3P algorithm yields multiple possible camera poses.  \n",
    "- To resolve this ambiguity, a fourth 2D-3D point correspondence is used to select the most accurate solution—specifically, the one with the lowest reprojection error.\n",
    "- This algorithm returns the rotation and translation matrices that yield the smallest reprojection error, along with the reprojection error values for the four points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poseidon.torch import find_best_solution_P3P_batch\n",
    "\n",
    "R_solution, C_solution, estimation_error = find_best_solution_P3P_batch(\n",
    "    solutions, points_2D, points_3D, A\n",
    ")\n",
    "\n",
    "print(\"\\033[1m Estimated Rotation Matrix \\033[0m (R_solution):\\n \", R_solution)\n",
    "print(\"\\033[1m Original Rotation Matrix \\033[0m  (R):\\n\", R, \"\\n\")\n",
    "print(\"\\033[1m Estimated Camera Position \\033[0m  (C_solution):\\n\", C_solution)\n",
    "print(\"\\033[1m Original Camera Position \\033[0m (C):\\n\", C, \"\\n\")\n",
    "print(\"Estimation Error:\\n\", estimation_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
